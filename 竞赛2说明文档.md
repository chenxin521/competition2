这次竞赛我真的感受到了很大的挫败，不同于以往80%，90%的正确率，这次竞赛我每一次提交的正确率以及大家的正确率都在30%多，极低的正确率使我明白了人工智能其实没有想象中的那么神奇，也明白了我对数据的敏感程度十分低，不知道如何合适地处理数据集是这次竞赛里最大的痛苦，同时在痛苦里也有了收获，这次竞赛我用KNN算法、神经网络softmax算法、以及从未接触过的ORC多分类算法尝试了对数据集进行分类但是正确率都不高，KNN算法正确率在0.3多，神经网络的正确率大致在0.3多，不知道为什么orc结合SVM二分类器实现数据多分类的正确率只有0.258多，可能是我对这个多分类算法理解还是不透彻。所以最终还是选择了比较容易实现的KNN算法，然后从数据上入手寻求突破口，但是我突破失败了。以下是我用KNN算法进行分类的代码实现过程：

以下是我的代码的实现过程。

1. 处理训练和测试用的数据

   调用numpy库、CSV库和sklearn中的SimpleImputer库来实现数据初始化。

```python
import numpy as np
import csv
from sklearn.impute import SimpleImputer
```

 	由于初始CSV文件里的数据含有字符串'?'，所以在读取数据后要对数据进行填补缺失值的处理，具体函数如下：

```python
##导入和初始化数据,补全缺省值
def init_data(dataDir):
    data = []
    with open(dataDir) as csvfile:
        csv_reader = csv.reader(csvfile)  
        for row in csv_reader:  
            data.append(row)

    #统计数据集的行数和列数
    row = 0
    colum = 0
    for i in data:
        row += 1
    for j in data[0]:
        colum += 1
     
    #将字符串？转化为缺失值np.nan
    for i in range(row):
        for j in range(colum):
            if(data[i][j] =='?'):
                data[i][j] = np.nan
     
    #将string型数据转化为float型，并把数据集转化为数组
    data = [[float(x) for x in row] for row in data]
    data = np.array(data)
    
    return data
    
def Compdefault(data):
    #用属性列的均值填补缺失值
    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')

    imp_mean.fit(data)
    newData = imp_mean.transform(data)
    
    return newData

```

① init_data(dataDir)函数：

(1)直接调用csv.reader函数对CSV文件按行读取，将读取到的每一行依次存储到data列表里，此时读取出来的每一个值都是string类型。

(2)然后写两个for循环，统计data数组的行数和列数，将行数值赋给row变量，将列数值赋给column变量。

(3)将data里面所有的'?'转化为缺失值np.nan.

(4)将data列表里面的所有值转化为float型，并将data转化为二维数组。

(5)返回data数组，data就是读取好初始化好的数据集。

② Compdefault(data)函数：

初始化一个SimpleImputer学习器imp_mean，学习器内的参数missing_values=np.nan表示要填补的数值是np.nan，参数strategy='mean'，代表用均值填补缺失值，默认用列的值的均值。我尝试过用众数most_frequent，众数median填补但是正确率也没有上去，可能是因为数据比较离散。

clf.fit(data)用于训练数据，clf.transform(data)用于处理数据。

```python
#在主函数中调用函数并划分类别和属性数组
def main():
    train_data = init_data('train.csv')
    test_data = init_data('test.csv')                                                

    trainData = train_data[:,:-1]
    classLabels = train_data[:,-1]
    classLabels = classLabels.ravel()
    
    trainData = Compdefault(trainData)
    test_data = Compdefault(test_data)
```

将训练集和测试集文件的文件路径传给init_data函数，此时训练集是包含类别的，要把初始化后的训练集划分，将最后一列划分出来作为类别标签数组classLabels，将剩余列作为属性数组trainData。把划分好的训练集和初始化后的测试集传入CompdeFault函数，填补缺失值，填补完成后就要开始训练了。



2. 训练数据，测试数据

   调用以下库实现KNN算法

   ```python
   from sklearn import neighbors
   from sklearn.neighbors import KNeighborsClassifier
   from sklearn import model_selection
   #from sklearn.model_selection import KFold
   from sklearn.metrics import accuracy_score
   ```

此次没有手写算法，直接调用sklearn中的KNN算法来训练和预测，发现直接调用库时weigths为uniform时对数据进行训练和预测，结果比weigths为distance更好一些，但是只是细微的差距。

```python
def Knn(trainData,classLabels,test_data):
    #将训练集划分，用于推导式选K值
    X_train,X_test,y_train,y_test = model_selection.train_test_split(trainData,classLabels, test_size=0.2,random_state=5)
                                                               
    #推导式选K值
    ks = [i for i in range(10,20)]
    for k in ks:
        clf = KNeighborsClassifier(n_neighbors = k,weights='uniform')
        clf.fit(trainData,classLabels)
        p = clf.predict(X_test)
        print(k,':',accuracy_score(y_test,p)) 
        
    clf = neighbors.KNeighborsClassifier(n_neighbors = 11,weights='uniform')
    clf.fit(trainData,classLabels)
    test_pred = clf.predict(test_data)
    
    #将数据整合成带序号的二维数组
    m = len(test_pred)
    prediction = [[] for i in range(m)]
    for i in range(m):  
        prediction[i].append(i+1)
        prediction[i].append(test_pred[i])
        
    # =============================================================================
#     #折线图
#     arrs = []
#     #推导式选K值
#     ks = [i for i in range(90,100)]
#     kz = KFold(n_splits = 5,shuffle = False)
#     for k in ks:
#         arr = []
#         for train_indexs,test_indexs in kz.split(X_train):
#             clf = KNeighborsClassifier(n_neighbors = k)
#             clf.fit(X_train[train_indexs],y_train[train_indexs])
#             p = clf.predict(X_train[test_indexs])
#             arr.append(1. - accuracy_score(y_train[test_indexs],p))
#         arrs.append(arr)
#     arrs = np.array(arrs)
#     mean_arrs = np.mean(arrs,axis = 1)
#     std_arrs = np.std(arrs,axis = 1)
#     plt.figure(figsize = (10,5))
#     plot_data = np.array([mean_arrs,mean_arrs+std_arrs,mean_arrs-std_arrs]).T
#     plt.plot(ks,plot_data,'.-')
# =============================================================================

    return prediction
```

Knn()函数：

（1）用sklearn中的model_selection库中的model_selection.train_test_split()函数，将训练集随机划分出20%作为验证集，以对比正确率挑选出最合适的K值。trainData，classLabels分别是训练集和训练集的类别标签的数组，作为要被划分的数组，testsize=0.2，代表将trainData，classLabels的20%划分出来作为新的验证集以及验证集的类别标签数组，random_state代表每次随机打乱数据集的随机种子数为5，选取5是经过多次测试的最优的。

(2)推导式确定一个范围内的K值，然后利用for循环将K为不同值时验证集的预测正确率输出，比较正确率。

最终确定在训练集为初始训练集trainData时，K为11时，预测正确率较高。

拓展：开始我觉得数据一直欠拟合，因为过拟合的话，增加数据集的量可以防止过拟合，所以我想在欠拟合的情况下可能缩小训练集正确率会较高，于是将原始训练集的80%也就是划分好的X_train数组作为训练集去观测正确率，然后利用五折交叉验证的办法画折线图观测正确率，选出来了86为最好的K值，但是提交后正确率最高达到0.35908，比最终提交的0.361...要低。

（3）初始化一个neighbors.KNeighborsClassifier()学习器clf，参数n_neighbors = 11代表K 值为11，参数weights='uniform'代表K个近邻是等权的，如果为distance的话那么K个近邻里距离要分类的点近的权重就会大一些。



